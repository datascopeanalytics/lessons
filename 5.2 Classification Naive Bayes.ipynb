{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We've learned a little bit about the theory of what Naive Bayes is and how it works already. Let's see some examples of using Naive Bayes.\n",
      "\n",
      "For starters, we'll use a well-known and ubiquitous data-set of hand-written digits."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import load_digits\n",
      "digits = load_digits()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What is this dataset?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print digits.DESCR"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Optical Recognition of Handwritten Digits Data Set\n",
        "\n",
        "Notes\n",
        "-----\n",
        "Data Set Characteristics:\n",
        "    :Number of Instances: 5620\n",
        "    :Number of Attributes: 64\n",
        "    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n",
        "    :Missing Attribute Values: None\n",
        "    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n",
        "    :Date: July; 1998\n",
        "\n",
        "This is a copy of the test set of the UCI ML hand-written digits datasets\n",
        "http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n",
        "\n",
        "The data set contains images of hand-written digits: 10 classes where\n",
        "each class refers to a digit.\n",
        "\n",
        "Preprocessing programs made available by NIST were used to extract\n",
        "normalized bitmaps of handwritten digits from a preprinted form. From a\n",
        "total of 43 people, 30 contributed to the training set and different 13\n",
        "to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n",
        "4x4 and the number of on pixels are counted in each block. This generates\n",
        "an input matrix of 8x8 where each element is an integer in the range\n",
        "0..16. This reduces dimensionality and gives invariance to small\n",
        "distortions.\n",
        "\n",
        "For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\n",
        "T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\n",
        "L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n",
        "1994.\n",
        "\n",
        "References\n",
        "----------\n",
        "  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n",
        "    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n",
        "    Graduate Studies in Science and Engineering, Bogazici University.\n",
        "  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n",
        "  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n",
        "    Linear dimensionalityreduction using relevance weighted LDA. School of\n",
        "    Electrical and Electronic Engineering Nanyang Technological University.\n",
        "    2005.\n",
        "  - Claudio Gentile. A New Approximate Maximal Margin Classification\n",
        "    Algorithm. NIPS. 2000.\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Also, we can use `__doc__` to find out more."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print load_digits.__doc__"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Load and return the digits dataset (classification).\n",
        "\n",
        "    Each datapoint is a 8x8 image of a digit.\n",
        "\n",
        "    =================   ==============\n",
        "    Classes                         10\n",
        "    Samples per class             ~180\n",
        "    Samples total                 1797\n",
        "    Dimensionality                  64\n",
        "    Features             integers 0-16\n",
        "    =================   ==============\n",
        "\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_class : integer, between 0 and 10, optional (default=10)\n",
        "        The number of classes to return.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    data : Bunch\n",
        "        Dictionary-like object, the interesting attributes are:\n",
        "        'data', the data to learn, 'images', the images corresponding\n",
        "        to each sample, 'target', the classification labels for each\n",
        "        sample, 'target_names', the meaning of the labels, and 'DESCR',\n",
        "        the full description of the dataset.\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    To load the data and visualize the images::\n",
        "\n",
        "        >>> from sklearn.datasets import load_digits\n",
        "        >>> digits = load_digits()\n",
        "        >>> print(digits.data.shape)\n",
        "        (1797, 64)\n",
        "        >>> import pylab as pl #doctest: +SKIP\n",
        "        >>> pl.gray() #doctest: +SKIP\n",
        "        >>> pl.matshow(digits.images[0]) #doctest: +SKIP\n",
        "        >>> pl.show() #doctest: +SKIP\n",
        "    \n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# see?\n",
      "print (digits.data.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(1797, 64)\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pylab as pl\n",
      "pl.gray()\n",
      "pl.matshow(digits.images[0])\n",
      "pl.show()\n",
      "# if you squint it's easier to see"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"The image we just saw was a %s.\" % digits.target[0]\n",
      "assert len(digits.target) == len(digits.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The image we just saw was a 0.\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So we have 8x8 images of hand-written digits. If we were the US Post Office in the 90's, we'd really love to be able to determine the zip-codes of all the mails that fly through our system. People are slow and error-prone; presumably a computer could do it better. But that means we need to be able to read in a noisy human-made real-world digit and determine what number it is.\n",
      "\n",
      "This dataset has already been preprocessed which you can read more about in the description `DESCR` above... (( sweeps under rug ))\n",
      "\n",
      "The first question we might want to ask is: what is the distribution of our dataset? We'll be training a classifier to learn what digit an image contains and it would be good to know that we have a little bit from each digit."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# these are the digits -- also called `labels` of the data\n",
      "print digits.target\n",
      "print digits.target.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0 1 2 ..., 8 9 8]\n",
        "(1797,)\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "np.bincount(digits.target)\n",
      "# that's pretty nicely distributed"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "array([178, 182, 177, 183, 181, 182, 181, 179, 174, 180])"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That looks pretty evenly distributed, which is good. We have about 180 examples per digit.\n",
      "\n",
      "To train and test our classifier, we'll want to make sure that we separate the data (otherwise we'll overfit)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import train_test_split\n",
      "train_indices, test_indices = train_test_split(\n",
      "                               np.arange(digits.target.shape[0]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now for the training. This is going to be really quick and if you blink you might miss it."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import naive_bayes\n",
      "\n",
      "clf = naive_bayes.GaussianNB().fit(\n",
      "            digits.data[train_indices], digits.target[train_indices])\n",
      "print \"%.2f\"% clf.score(digits.data[test_indices], \n",
      "                        digits.target[test_indices])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.82\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "78% accuracy is not bad, but this assumes that the distribution $P($ this pixel is dark-grey $|$ this image is a 9$)$ is a Gaussian distributed variable. \n",
      "\n",
      "This distribution is not a great choice for this data  -- since the variable takes integer values between 0 and 16, corresponding to pixels from light to dark. \n",
      "\n",
      "In the preprocessing step the image was rendered from a 256x256bit image (a boolean matrix of size 256x256) down to an 8x8 matrix in which each cell represents 16 cells in the original. That means that each cell could be modeled as a Bernoulli random variable (if each cell in the original had identical distributions associated to them) or a Multionomially random variable (corresponding to each cell in the original having varying distributions).\n",
      "\n",
      "Thankfully, each of these models is a sinch to test."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def score(clf):\n",
      "    return clf.fit(digits.data[train_indices], \n",
      "                   digits.target[train_indices]\n",
      "                   ).score(digits.data[test_indices], \n",
      "                           digits.target[test_indices])\n",
      "print \"%.2f\"%score(naive_bayes.BernoulliNB())\n",
      "print \"%.2f\"%score(naive_bayes.MultinomialNB())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.84\n",
        "0.90\n"
       ]
      }
     ],
     "prompt_number": 11
    }
   ],
   "metadata": {}
  }
 ]
}